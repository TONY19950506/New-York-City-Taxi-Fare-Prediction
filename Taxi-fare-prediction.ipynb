{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-12T15:39:50.731548Z","iopub.status.busy":"2023-05-12T15:39:50.731058Z","iopub.status.idle":"2023-05-12T15:39:50.770887Z","shell.execute_reply":"2023-05-12T15:39:50.769658Z","shell.execute_reply.started":"2023-05-12T15:39:50.731504Z"}},"source":["# NY Taxi Fare Prediction\n","\n","## Final Competition - Group 34\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T11:06:25.044174Z","iopub.status.busy":"2023-05-13T11:06:25.043591Z","iopub.status.idle":"2023-05-13T11:06:25.090040Z","shell.execute_reply":"2023-05-13T11:06:25.089076Z","shell.execute_reply.started":"2023-05-13T11:06:25.044126Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datetime as dt\n","import xgboost as xgb\n","from catboost import CatBoostRegressor\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","\n","from typing import Callable\n","\n","import os\n","# input_dir = '/kaggle/input/'\n","# working_dir = '/kaggle/working/'\n","input_dir = r'.\\input'\n","working_dir = r'.\\working'\n","WEATHER = True\n","TAXICAB = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T11:06:35.528546Z","iopub.status.busy":"2023-05-13T11:06:35.528179Z","iopub.status.idle":"2023-05-13T11:06:52.969171Z","shell.execute_reply":"2023-05-13T11:06:52.967959Z","shell.execute_reply.started":"2023-05-13T11:06:35.528518Z"},"trusted":true},"outputs":[],"source":["# Reading Training Data\n","N_ROWS = 30_000_000\n","df_train = pd.read_csv(os.path.join(input_dir, 'new-york-city-taxi-fare-prediction/train.csv'),\n","                       nrows=N_ROWS, parse_dates=[\"pickup_datetime\"])\n","if TAXICAB:\n","    df_taxi = pd.read_csv(os.path.join(input_dir, 'taxicab_dis_train_85000.csv'),\n","                        nrows=N_ROWS)\n","    df_train = pd.concat([df_train, df_taxi], axis= 1)\n","\n","df_train_copy = df_train.copy()\n","df_train.head()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preprocessing\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data Cleansing (For Training Data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T11:07:00.235660Z","iopub.status.busy":"2023-05-13T11:07:00.235257Z","iopub.status.idle":"2023-05-13T11:07:00.246200Z","shell.execute_reply":"2023-05-13T11:07:00.244877Z","shell.execute_reply.started":"2023-05-13T11:07:00.235627Z"},"trusted":true},"outputs":[],"source":["# remove any data with out-of-bound pickup or dropoff location (coordinate outside NYC)\n","def remove_out_of_bound(df_input: pd.DataFrame,\n","                        longitude_bounds: list = [-75, -72],\n","                        latitude_bounds: list = [40, 42]) -> pd.DataFrame:\n","    pickup_in_bound = ((df_input.pickup_longitude > longitude_bounds[0]) &\n","                       (df_input.pickup_longitude < longitude_bounds[1]) &\n","                       (df_input.pickup_latitude > latitude_bounds[0]) &\n","                       (df_input.pickup_latitude < latitude_bounds[1]))\n","    dropoff_in_bound = ((df_input.dropoff_longitude > longitude_bounds[0]) &\n","                        (df_input.dropoff_longitude < longitude_bounds[1]) &\n","                        (df_input.dropoff_latitude > latitude_bounds[0]) &\n","                        (df_input.dropoff_latitude < latitude_bounds[1]))\n","    return df_input[pickup_in_bound & dropoff_in_bound]\n","\n","# remove any data negative fare\n","\n","\n","def remove_negative_fare(df_input: pd.DataFrame) -> pd.DataFrame:\n","    return df_input.drop(df_input[df_input['fare_amount'] < 0].index, axis=0)\n","\n","# remove any data with unrealistic passenger count\n","\n","\n","def limit_max_passenger(df_input: pd.DataFrame, max_passenger: int = 6) -> pd.DataFrame:\n","    return df_input.drop(df_input[df_input['passenger_count'] > max_passenger].index, axis=0)\n","\n","# preprocessing pipeline containing all cleansing steps\n","\n","\n","def remove_all_invalid_data(df_input: pd.DataFrame) -> pd.DataFrame:\n","    print('Before Cleansing: {}'.format(len(df_input)))\n","    df_input = df_input.dropna(how='any', axis='rows')\n","    print('After Removing NaN values: {}'.format(len(df_input)))\n","    df_input = remove_out_of_bound(df_input)\n","    print('After Removing Out-of-Bounds: {}'.format(len(df_input)))\n","    df_input = remove_negative_fare(df_input)\n","    print('After Removing Negative Fares: {}'.format(len(df_input)))\n","    df_input = limit_max_passenger(df_input)\n","    print('After Limiting Max Passengers: {}'.format(len(df_input)))\n","    return df_input\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Feature Engineering\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Distance Features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T11:07:03.867440Z","iopub.status.busy":"2023-05-13T11:07:03.867066Z","iopub.status.idle":"2023-05-13T11:07:03.878001Z","shell.execute_reply":"2023-05-13T11:07:03.876517Z","shell.execute_reply.started":"2023-05-13T11:07:03.867411Z"},"trusted":true},"outputs":[],"source":["# linear distance between pickup and dropoff location\n","def euclidean_dist(pickup_lat: float, pickup_long: float,\n","                   dropoff_lat: float, dropoff_long: float) -> float:\n","    LAT2KM = 110.574\n","    LONG2KM = 111.320\n","    distance = np.sqrt(((dropoff_lat - pickup_lat) * LAT2KM) ** 2 +\n","                       ((dropoff_long - pickup_long) * LONG2KM) ** 2)\n","    return distance\n","\n","# shortest distance between pickup and dropoff location on a sphere\n","\n","\n","def haversine_dist(pickup_lat: float, pickup_long: float,\n","                   dropoff_lat: float, dropoff_long: float) -> float:\n","    dLat = (dropoff_lat - pickup_lat) * np.pi / 180.0\n","    dLon = (dropoff_long - pickup_long) * np.pi / 180.0\n","    lat1 = (pickup_lat) * np.pi / 180.0\n","    lat2 = (dropoff_lat) * np.pi / 180.0\n","\n","    a = (np.power(np.sin(dLat / 2), 2) +\n","         np.power(np.sin(dLon / 2), 2) *\n","         np.cos(lat1) * np.cos(lat2))\n","    RAD = 6371\n","    distance = 2 * np.arcsin(np.sqrt(a))\n","    return RAD * distance\n","\n","\n","def get_distance(df_input: pd.DataFrame,\n","                 distance_func: Callable[[float, float, float, float], float]) -> pd.DataFrame:\n","    return distance_func(df_input['pickup_latitude'], df_input['pickup_longitude'], df_input['dropoff_latitude'], df_input['dropoff_longitude'])\n","\n","    '''\n","    return df_input.apply(lambda row :distance_func(row['pickup_latitude'],\n","                                                     row['pickup_longitude'],\n","                                                     row['dropoff_latitude'],\n","                                                     row['dropoff_longitude'],), axis = 1)\n","    '''\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Time and Date Features\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# split datetime datatype to separate columns\n","def get_date_time(df_input: pd.DataFrame, time_col_name: str = 'pickup_datetime') -> pd.DataFrame:\n","    df_datetime = pd.DataFrame()\n","    df_datetime['year'] = df_input[time_col_name].dt.year\n","    df_datetime['month'] = df_input[time_col_name].dt.month\n","    df_datetime['date'] = df_input[time_col_name].dt.day\n","    df_datetime['day_of_week'] = df_input[time_col_name].dt.dayofweek\n","    df_datetime['hour'] = df_input[time_col_name].dt.hour\n","    return df_datetime\n","\n","# encode cyclic features into sine and consine components\n","\n","\n","def cyclical_encoding(df_input: pd.DataFrame, cyclic_features: list = ['month', 'date', 'day_of_week', 'hour']) -> pd.DataFrame:\n","    max_val_dict = {'month': 12, 'date': 31, 'day_of_week': 7, 'hour': 23}\n","    df_cyclic = pd.DataFrame()\n","    for feature in cyclic_features:\n","        df_cyclic[feature +\n","                  '_sin'] = np.sin(2 * np.pi * df_input[feature]/max_val_dict[feature])\n","        df_cyclic[feature +\n","                  '_cos'] = np.cos(2 * np.pi * df_input[feature]/max_val_dict[feature])\n","    return df_cyclic\n","\n","\n","def is_after_price_hike(df_input: pd.DataFrame) -> pd.DataFrame:\n","    PRICE_HIKE_DAY = dt.datetime(2012, 10, 1, 0, 0, 0, 0, dt.timezone.utc)\n","    return df_input.pickup_datetime > PRICE_HIKE_DAY\n","\n","\n","def is_rush_hour(df_input: pd.DataFrame) -> pd.DataFrame:\n","    rush_hour = (((df_input['hour'] > 15) & (df_input['hour'] < 20)) & (\n","        (df_input['day_of_week'] > 0) & (df_input['day_of_week'] < 6)))\n","    return rush_hour\n","\n","\n","def is_overnight(df_input: pd.DataFrame) -> pd.DataFrame:\n","    overnight = (((df_input['hour'] > 21) | (df_input['day_of_week'] < 6)))\n","    return overnight\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Location-Based Features\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_dist_to_airports(df_input: pd.DataFrame) -> pd.DataFrame:\n","    jfk_coord = (40.639722, -73.778889)\n","    ewr_coord = (40.6925, -74.168611)\n","    lga_coord = (40.77725, -73.872611)\n","\n","    pickup_lat = df_input['pickup_latitude']\n","    dropoff_lat = df_input['dropoff_latitude']\n","    pickup_lon = df_input['pickup_longitude']\n","    dropoff_lon = df_input['dropoff_longitude']\n","\n","    pickup_jfk = haversine_dist(\n","        pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1])\n","    dropoff_jfk = haversine_dist(\n","        jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon)\n","    pickup_ewr = haversine_dist(\n","        pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\n","    dropoff_ewr = haversine_dist(\n","        ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon)\n","    pickup_lga = haversine_dist(\n","        pickup_lat, pickup_lon, lga_coord[0], lga_coord[1])\n","    dropoff_lga = haversine_dist(\n","        lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon)\n","\n","    df_distances = pd.DataFrame()\n","\n","    df_distances['jfk_dist'] = pd.concat(\n","        [pickup_jfk, dropoff_jfk], axis=1).min(axis=1)\n","    df_distances['ewr_dist'] = pd.concat(\n","        [pickup_ewr, dropoff_ewr], axis=1).min(axis=1)\n","    df_distances['lga_dist'] = pd.concat(\n","        [pickup_lga, dropoff_lga], axis=1).min(axis=1)\n","\n","    return df_distances\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Utilities\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def remove_duplicate_columns(df_input: pd.DataFrame) -> pd.DataFrame:\n","    return df_input.loc[:, ~df_input.columns.duplicated()]\n","\n","\n","def add_all_features(df_input: pd.DataFrame) -> pd.DataFrame:\n","    # distance features\n","    df_input['euclidean_distance'] = get_distance(df_input, euclidean_dist)\n","    df_input['haversine_distance'] = get_distance(df_input, haversine_dist)\n","\n","    # time features\n","    df_input = remove_duplicate_columns(\n","        pd.concat([df_input, get_date_time(df_input)], axis=1))\n","    df_input = remove_duplicate_columns(\n","        pd.concat([df_input, cyclical_encoding(df_input)], axis=1))\n","    \n","    # time and regulation features\n","    df_input['is_after_price_hike'] = is_after_price_hike(df_input)\n","    df_input['is_rush_hour'] = is_rush_hour(df_input)\n","    df_input['is_overnight'] = is_overnight(df_input)\n","    \n","    # airport vicinity features\n","    df_input = remove_duplicate_columns(\n","        pd.concat([df_input, get_dist_to_airports(df_input)], axis=1))\n","    \n","    return df_input\n","\n","def standard_scaling(df_input: pd.DataFrame, scaler= StandardScaler()) -> pd.DataFrame:\n","    df_scaled_input = scaler.transform(df_input)\n","    return pd.DataFrame(df_scaled_input, columns = df_input.columns)   "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Main Code\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for quick start\n","df_train = df_train_copy.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if (WEATHER == True):\n","    # get weather features\n","    df_weather = pd.read_csv(os.path.join(input_dir, 'open-meteo/weather.csv'),\n","                            parse_dates=[\"time\"])\n","    df_weather = remove_duplicate_columns(\n","        pd.concat([df_weather, get_date_time(df_weather, time_col_name= 'time')], axis=1))\n","    df_weather.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T11:07:06.720351Z","iopub.status.busy":"2023-05-13T11:07:06.719964Z","iopub.status.idle":"2023-05-13T11:07:06.780487Z","shell.execute_reply":"2023-05-13T11:07:06.779612Z","shell.execute_reply.started":"2023-05-13T11:07:06.720322Z"},"trusted":true},"outputs":[],"source":["# Preprocessing\n","df_train = remove_all_invalid_data(df_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T11:07:09.364170Z","iopub.status.busy":"2023-05-13T11:07:09.363562Z","iopub.status.idle":"2023-05-13T11:07:18.119582Z","shell.execute_reply":"2023-05-13T11:07:18.118381Z","shell.execute_reply.started":"2023-05-13T11:07:09.364135Z"},"trusted":true},"outputs":[],"source":["# Feature Engineering\n","# distance features\n","df_train = add_all_features(df_train)\n","\n","if WEATHER == True:\n","    df_train =  pd.merge(df_train, df_weather, on=['year', 'month', 'date', 'day_of_week', 'hour'])\n","    df_train['weathercode'] = df_train['weathercode'].astype('category')\n","\n","df_train.columns"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fixing Broken Taxicab Distance\n","if TAXICAB:\n","    taxi_NA_idx = (df_train['taxicab_distance']==-1) | (df_train['taxicab_distance']==0)\n","    df_train.loc[taxi_NA_idx, 'taxicab_distance'] = df_train['euclidean_distance'][taxi_NA_idx]*1000"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training\n","df_train_keys = df_train['key']\n","df_train_Y = df_train['fare_amount']\n","\n","unnecessary_cols = ['key', 'pickup_datetime', 'fare_amount']\n","if WEATHER == True:\n","    unnecessary_cols.append('time')\n","df_train_X = df_train.drop(unnecessary_cols, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler = StandardScaler()\n","scaler.fit(df_train_X)\n","df_train_X = standard_scaling(df_train_X, scaler)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["    'learning_rate': [0.01, 0.05, 0.12, 0.3],\n","    'max_depth': [6, 7, 8],\n","    'min_child_weight': [1, 5, 10],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0],\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# hyperparameters optimization\n","FIND_HYPERPARAMETER = False\n","if FIND_HYPERPARAMETER:\n","    xgb_model = xgb.XGBRegressor()\n","    parameters = {\n","        # tunable parameters\n","        'learning_rate': [0.05],\n","        'max_depth': [5, 8, 10],\n","        'min_child_weight': [1, 5, 15, 200],\n","        'subsample': np.arange(0.5, 1.0, 0.1),\n","        'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n","        'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n","        'n_estimators': [100, 500, 1000],\n","\n","\n","        # untunable parameters\n","        'objective': ['reg:squarederror'],\n","        'nthread': [8],\n","        'predictor': ['gpu_predictor'],\n","        'verbosity': [0],\n","        'seed': [1]\n","    }\n","\n","    clf = RandomizedSearchCV(xgb_model, param_distributions=parameters, n_jobs=1, n_iter=25,\n","                             scoring='neg_root_mean_squared_error', verbose=5, refit=True)\n","\n","    clf.fit(df_train_X, df_train_Y)\n","    cv_best_param = clf.best_params_\n","    xgb_model_best = clf.best_estimator_\n","    print(cv_best_param)\n","\n","''' \n","best params\n","{'colsample_bytree': 0.6,\n"," 'colsample_bylevel': 0.7,\n"," 'learning_rate': 0.05,\n"," 'max_depth': 8,\n"," 'min_child_weight': 1,\n"," 'n_estimators': 1000,\n"," 'nthread': 8,\n"," 'objective': 'reg:squarederror',\n"," 'predictor': 'gpu_predictor',\n"," 'seed': 1,\n"," 'subsample': 1.0,\n"," 'verbosity': 0}\n","'''\n","'''\n","{'verbosity': 0, 'subsample': 0.8999999999999999, 'seed': 1, 'predictor': 'gpu_predictor', 'objective': 'reg:squarederror', 'nthread': 8, 'n_estimators': 1000, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bytree': 0.6, 'colsample_bylevel': 0.7}\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    df_train_X, df_train_Y, random_state=56, test_size=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training\n","best_param = {'colsample_bytree': 0.8,\n","              'colsample_bylevel': 0.8,\n","              'learning_rate': 0.01,\n","              'max_depth': 8,\n","              'min_child_weight': 1,\n","              'n_estimators': 1000,\n","              'subsample': 0.9,\n","              \n","              'nthread': 8,\n","              'objective': 'reg:squarederror',\n","              'predictor': 'gpu_predictor',\n","              'seed': 1,\n","              'subsample': 0.8,\n","              'booster': 'gbtree',\n","              'verbosity': 0}\n","\n","dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical= True)\n","dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical= True)\n","\n","n = 10000\n","model = xgb.train(\n","    params=best_param,\n","    dtrain=dtrain_reg,\n","    num_boost_round=n,\n","    early_stopping_rounds=50,\n","\n","    evals=[(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n",")\n","\n","preds = model.predict(dtest_reg)\n","rmse = mean_squared_error(y_test, preds, squared=False)\n","print(rmse)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FIND_CAT_HYPERPARAMETER = False\n","\n","if FIND_CAT_HYPERPARAMETER:\n","        grid = {'depth': [6, 10, 14],\n","                'l2_leaf_reg': [0.5, 5, 15, 30]}\n","\n","        cat_model = CatBoostRegressor(random_state=3,\n","                                l2_leaf_reg=30,\n","                                iterations=10000,\n","                                early_stopping_rounds=50,\n","                                eval_metric='RMSE',\n","                                # use_best_model=True,\n","                                task_type=\"GPU\")\n","        cat_model.grid_search(grid, X_train, y_train,\n","                        search_by_train_test_split=True, plot=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CATBOOST = False\n","if CATBOOST:\n","        grid = {'depth': [6, 10, 14],\n","                'l2_leaf_reg': [0.5, 5, 15, 30]}\n","\n","        cat_model = CatBoostRegressor(random_state=3,\n","                                depth=10,\n","                                l2_leaf_reg=30,\n","                                iterations=10000,\n","                                early_stopping_rounds=50,\n","                                eval_metric='RMSE',\n","                                use_best_model=True,\n","                                task_type=\"GPU\")\n","\n","        cat_model.fit(X_train,y_train, eval_set=(X_test, y_test), verbose=1, plot=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CONTINUE = False\n","# continue training (optional)\n","if CONTINUE:\n","    n = 10000\n","    model = xgb.train(\n","        params=best_param,\n","        dtrain=dtrain_reg,\n","        num_boost_round=n,\n","        early_stopping_rounds=100,\n","        xgb_model=model,\n","        evals=[(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n","    )\n","\n","    preds = model.predict(dtest_reg)\n","    rmse = mean_squared_error(y_test, preds, squared=False)\n","    print(rmse)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test = pd.read_csv(os.path.join(\n","    input_dir, 'new-york-city-taxi-fare-prediction/test.csv'), parse_dates=[\"pickup_datetime\"])\n","if TAXICAB:\n","    df_taxi = pd.read_csv(os.path.join(input_dir, 'taxicab_dis_test.csv'),)\n","    df_test = pd.concat([df_test, df_taxi], axis= 1)\n","df_test.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test = add_all_features(df_test)\n","if WEATHER == True:\n","    df_test =  pd.merge(df_test, df_weather, on=['year', 'month', 'date', 'day_of_week', 'hour'])\n","    df_test['weathercode'] = df_test['weathercode'].astype('category')\n","\n","\n","df_test_keys = df_test['key']\n","unnecessary_cols = ['key', 'pickup_datetime']\n","if WEATHER == True:\n","    unnecessary_cols.append('time')\n","df_test_X = df_test.drop(unnecessary_cols, axis=1)\n","\n","df_test_X = standard_scaling(df_test_X, scaler)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test_X"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-13T11:07:24.415854Z","iopub.status.busy":"2023-05-13T11:07:24.415421Z","iopub.status.idle":"2023-05-13T11:07:24.424035Z","shell.execute_reply":"2023-05-13T11:07:24.421766Z","shell.execute_reply.started":"2023-05-13T11:07:24.415819Z"},"trusted":true},"outputs":[],"source":["df_test_Y = model.predict(xgb.DMatrix(df_test_X))\n","df_test_submit = pd.DataFrame()\n","df_test_submit['key'] = df_test_keys\n","df_test_submit['fare_amount'] = df_test_Y\n","df_test_submit\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if CATBOOST:\n","    df_test_Y = cat_model.predict(df_test_X)\n","    df_test_submit = pd.DataFrame()\n","    df_test_submit['key'] = df_test_keys\n","    df_test_submit['fare_amount'] = df_test_Y\n","    df_test_submit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test_submit.to_csv(os.path.join(\n","    working_dir, '010_submission.csv'), index=False)\n","model.save_model(os.path.join(working_dir, '010.model'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","configs = json.loads(model.save_config())\n","configs['learner']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot\n","\n","feature_importance = model.get_score(importance_type='gain')\n","# feature importance\n","print(feature_importance)\n","\n","# plot\n","keys = list(feature_importance.keys())\n","values = list(feature_importance.values())\n","\n","data = pd.DataFrame(data=values, index=keys, columns=[\n","                    \"score\"]).sort_values(by=\"score\", ascending=False)\n","data.nlargest(50, columns=\"score\").plot(kind='barh', figsize=(20, 10))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
